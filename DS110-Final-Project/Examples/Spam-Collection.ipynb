{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ce24b6",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### About Dataset\n",
    "\n",
    "#### Context\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "\n",
    "#### Content\n",
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "\n",
    "This corpus has been collected from free or free for research sources at the Internet:\n",
    "\n",
    "-> A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link](http://www.grumbletext.co.uk/).\n",
    "\n",
    "-> A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link](http://www.comp.nus.edu.sg/~rpnlpir/downloads/corpora/smsCorpus/).\n",
    "\n",
    "-> A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link](http://etheses.bham.ac.uk/253/1/Tagg09PhD.pdf).\n",
    "\n",
    "-> Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link](http://www.esp.uem.es/jmgomez/smsspamcorpus/). This corpus has been used in the following academic researches:\n",
    "\n",
    "#### Acknowledgements\n",
    "The original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection). The creators would like to note that in case you find the dataset useful, please make a reference to previous paper and the web page: http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers, research, etc.\n",
    "\n",
    "We offer a comprehensive study of this corpus in the following paper. This work presents a number of statistics, studies and baseline results for several machine learning methods.\n",
    "\n",
    "Almeida, T.A., GÃ³mez Hidalgo, J.M., Yamakami, A. Contributions to the Study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (DOCENG'11), Mountain View, CA, USA, 2011.\n",
    "\n",
    "Inspiration\n",
    "Can you use this dataset to build a prediction model that will accurately classify which texts are spam?\n",
    "\n",
    "\n",
    "[Reference](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c039507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6fdac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fcb756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3db443",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18e7a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"../Data/SMSSpamCollections.csv\", sep = \"\\t\",names=[\"Label\", \"Message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07d8814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Label  Message\n",
      "0  ham,\"Go until jurong point, crazy.. Available ...      NaN\n",
      "1                  ham,Ok lar... Joking wif u oni...      NaN\n",
      "2  spam,Free entry in 2 a wkly comp to win FA Cup...      NaN\n",
      "3  ham,U dun say so early hor... U c already then...      NaN\n",
      "4  ham,\"Nah I don't think he goes to usf, he live...      NaN\n"
     ]
    }
   ],
   "source": [
    "print(spam.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddc5f61",
   "metadata": {},
   "source": [
    "### Converting the DataFrame into List of tuples: Each tuple contains a labal and a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10adf7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for index, row in spam.iterrows():\n",
    "    data_set.append((row['Message'], row['Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e35d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(nan, 'ham,\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"'), (nan, 'ham,Ok lar... Joking wif u oni...'), (nan, \"spam,Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"), (nan, 'ham,U dun say so early hor... U c already then say...'), (nan, 'ham,\"Nah I don\\'t think he goes to usf, he lives around here though\"')]\n"
     ]
    }
   ],
   "source": [
    "print(data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64acc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25912a1e",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b51e4b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931dbc73",
   "metadata": {},
   "source": [
    "##### Changes document to lower case, remove stopwords and lemmatizes/stems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1859e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(document, stem=True):\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "        \n",
    "    document = \" \".join(words)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d185fc",
   "metadata": {},
   "source": [
    "##### Preprocessing on data_set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42447560",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m messages_set \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (message, label) \u001b[38;5;129;01min\u001b[39;00m data_set:\n\u001b[0;32m----> 3\u001b[0m     words_filtered \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      4\u001b[0m     messages_set\u001b[38;5;241m.\u001b[39mappend((words_filtered, label))\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(document, stem)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(document, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[43mdocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m      3\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(document)\n\u001b[1;32m      4\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "messages_set = []\n",
    "for (message, label) in data_set:\n",
    "    words_filtered = [e.lower() for e in preprocess(message, stem=False).split() if len(e) >=3]\n",
    "    messages_set.append((words_filtered, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7959814a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['jurong', 'point', 'crazy', 'available', 'bugis', 'great', 'world', 'buffet', '...', 'cine', 'get', 'amore', 'wat', '...'], 'ham'), (['lar', '...', 'joke', 'wif', 'oni', '...'], 'ham'), (['free', 'entry', 'wkly', 'comp', 'win', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', '87121', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', '08452810075over18'], 'spam'), (['dun', 'say', 'early', 'hor', '...', 'already', 'say', '...'], 'ham'), (['nah', \"n't\", 'think', 'usf', 'live', 'around', 'though'], 'ham')]\n"
     ]
    }
   ],
   "source": [
    "print(messages_set[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780ccb4",
   "metadata": {},
   "source": [
    "### Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13db7d8",
   "metadata": {},
   "source": [
    "##### Single list of words in the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58eadc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_messages(messages):\n",
    "    all_words = []\n",
    "    for (message, label) in messages:\n",
    "        all_words.extend(message)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5a089",
   "metadata": {},
   "source": [
    "##### Final feature list using intuitive FreqDist, to eliminate all the duplicate words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0797b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c05e8af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8003\n"
     ]
    }
   ],
   "source": [
    "word_features = get_word_features(get_words_in_messages(messages_set))\n",
    "print(len(word_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e54d24",
   "metadata": {},
   "source": [
    "### Creating a Train and Test Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56063c",
   "metadata": {},
   "source": [
    "##### Creating slicing index at 80% threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5633b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliceIndex = int((len(messages_set) * .8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bddc90",
   "metadata": {},
   "source": [
    "##### Shuffling the pack to create a random and unbiased split of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a268f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages, test_messages = messages_set[:sliceIndex], messages_set[sliceIndex:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81dfcb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_messages)\n",
    "len(test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b880fa",
   "metadata": {},
   "source": [
    "### Creating Feature Maps for Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0aa3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%5)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041b5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, train_messages)\n",
    "testing_set = nltk.classify.apply_features(extract_features, test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4491f51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  4457\n",
      "Test set size:  1115\n"
     ]
    }
   ],
   "source": [
    "print('Training set size: ', len(training_set))\n",
    "print('Test set size: ', len(testing_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577b626",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ead98bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nc/y19k6d7d11d0l15v8bbfnft40000gn/T/ipykernel_1175/1985653257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train[training_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2678682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
